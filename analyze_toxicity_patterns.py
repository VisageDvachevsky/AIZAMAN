#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""–ê–Ω–∞–ª–∏–∑ –ü–ê–¢–¢–ï–†–ù–û–í –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏"""

import pandas as pd
import torch
import re
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from collections import Counter

print("üîß –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏...")
model_name = 'textdetox/xlmr-large-toxicity-classifier-v2'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
model.eval()

df = pd.read_csv('submission.tsv', sep='\t')

print('\nüîç –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ—Å—Ç–∞—Ç–æ—á–Ω–æ–π —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏:\n')

# –ù–∞—Ö–æ–¥–∏–º –í–°–ï –ø—Ä–∏–º–µ—Ä—ã –≥–¥–µ –¥–µ—Ç–æ–∫—Å —Ç–æ–∫—Å–∏—á–µ–Ω
toxic_detox = []
word_patterns = Counter()

for idx, row in df.iterrows():
    detox = str(row['tat_detox1'])

    inputs = tokenizer(detox, return_tensors='pt', truncation=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1)
        toxic_prob = probs[0][1].item()

    if toxic_prob > 0.3:  # –¢–æ–∫—Å–∏—á–Ω—ã–π
        toxic_detox.append((idx, toxic_prob, detox))
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å–ª–æ–≤–∞
        words = re.findall(r'[–∞-—è”ô”©“Ø“£“ì“õ“ª—ëa-z”ô”©“Ø“£“ì“õ“ª]+', detox.lower())
        word_patterns.update(words)

print(f'üìä –ù–∞–π–¥–µ–Ω–æ {len(toxic_detox)} —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –¥–µ—Ç–æ–∫—Å–æ–≤ –∏–∑ {len(df)}')
print(f'   –ü—Ä–æ—Ü–µ–Ω—Ç: {len(toxic_detox)/len(df)*100:.1f}%\n')

# –¢–æ–ø —Å–ª–æ–≤ –≤ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –¥–µ—Ç–æ–∫—Å–∞—Ö
print('üéØ –¢–æ–ø-40 —Å–ª–æ–≤ –≤ –¢–û–ö–°–ò–ß–ù–´–• –¥–µ—Ç–æ–∫—Å–∞—Ö (–ø–∞—Ç—Ç–µ—Ä–Ω—ã):')
print('   (—ç—Ç–∏ —Å–ª–æ–≤–∞ —á–∞—Å—Ç–æ –æ—Å—Ç–∞—é—Ç—Å—è –∏ –º–æ–¥–µ–ª—å –∏—Ö —Å—á–∏—Ç–∞–µ—Ç —Ç–æ–∫—Å–∏—á–Ω—ã–º–∏)\n')

for word, count in word_patterns.most_common(40):
    if len(word) >= 3:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–ª–æ–≥–∏
        print(f'  {word}: {count} —Ä–∞–∑')

print('\nüìã –ü—Ä–∏–º–µ—Ä—ã —Ç–æ–∫—Å–∏—á–Ω—ã—Ö –¥–µ—Ç–æ–∫—Å–æ–≤ (–ø–µ—Ä–≤—ã–µ 10):')
for idx, prob, text in toxic_detox[:10]:
    print(f'  [{idx}] P={prob:.2f}: {text[:80]}')

print("\n‚úÖ –ì–æ—Ç–æ–≤–æ! –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–∏ –ü–ê–¢–¢–ï–†–ù–´ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è.")
