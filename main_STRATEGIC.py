#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ –°–¢–†–ê–¢–ï–ì–ò–ß–ï–°–ö–û–ï –†–ï–®–ï–ù–ò–ï - –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–ª–∞–Ω–∞ —É–ª—É—á—à–µ–Ω–∏—è –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

–†–ï–ê–õ–ò–ó–û–í–ê–ù–ù–´–ï –§–ê–ó–´:
‚úÖ –§–∞–∑–∞ 1: Chain-of-Thought Reasoning + –¢–∞—Ç–∞—Ç–∞—Ä—Å–∫–∞—è —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ—Å—Ç—å + –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
‚úÖ –§–∞–∑–∞ 2: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Ç–æ–∫—Å–∏—á–Ω—ã–π –ª–µ–∫—Å–∏–∫–æ–Ω —Å –º–∞—Ä–∫–∏—Ä–æ–≤–∫–æ–π
‚úÖ –§–∞–∑–∞ 3: Multi-candidate generation + Reranking —Å LaBSE
‚úÖ –§–∞–∑–∞ 4: Few-shot examples + –î–≤—É—Ö–ø—Ä–æ—Ö–æ–¥–Ω–∞—è –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏—è

–û–ñ–ò–î–ê–ï–ú–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢: J-score 0.70-0.75+
"""

import re
import os
import pandas as pd
from openai import OpenAI
from tqdm import tqdm
import time
from google import genai
from functools import lru_cache
from typing import List, Tuple, Optional

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ù–ê–°–¢–†–û–ô–ö–ò
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# API_KEY = os.getenv("OPENAI_API_KEY", "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")
# MODEL_NAME = "gpt-4o-2024-11-20"  # –õ—É—á—à–∏–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç—å/–∫–∞—á–µ—Å—Ç–≤–æ
client = genai.Client(api_key=os.getenv("GOOGLE_API_KEY", "AIzaSyBfyFXLPDYTBzDMeicbODHZ45KM8ARMUMY"))
GEMINI_MODEL = "gemini-2.5-flash" 
NUM_CANDIDATES = 3  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

# client = OpenAI(api_key=API_KEY)

INPUT_FILE = "dev_inputs.tsv"
OUTPUT_FILE = "submission_strategic.tsv"

# –°—á–µ—Ç—á–∏–∫–∏
total_api_calls = 0
total_input_tokens = 0
total_output_tokens = 0

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –§–ê–ó–ê 2: –†–ê–°–®–ò–†–ï–ù–ù–´–ô –¢–û–ö–°–ò–ß–ù–´–ô –õ–ï–ö–°–ò–ö–û–ù
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê


from google import genai
from google.genai import types

import time
import json

def gemini_generate(prompt: str, temperature: float, seed: int = 0, max_tokens: int = 300) -> str:
    global total_api_calls

    while True:
        try:
            response = client.models.generate_content(
                model=GEMINI_MODEL,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=float(temperature),
                    max_output_tokens=int(max_tokens),
                    seed=int(seed),
                )
            )
            total_api_calls += 1
            return (response.text or "").strip()

        except Exception as e:
            msg = str(e)
            # –≥—Ä—É–±—ã–π –ø–∞—Ä—Å 429 —Å RetryInfo
            if "RESOURCE_EXHAUSTED" in msg or "429" in msg:
                print(f"‚ö†Ô∏è Rate limit, —Å–ø–∏–º 20s: {e}")
                time.sleep(20)   # –º–æ–∂–Ω–æ –≤–∑—è—Ç—å 16‚Äì20s —Å –∑–∞–ø–∞—Å–æ–º
                continue
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Gemini: {e}")
            return ""




TATAR_TOXIC_LEXICON = {
    # –Ø–≤–Ω–∞—è –æ–±—Å—Ü–µ–Ω–Ω–∞—è –ª–µ–∫—Å–∏–∫–∞ (–∏–∑ —Ä—É—Å—Å–∫–æ–≥–æ)
    '–±–ª—è', '–±–ª—ç—Ç', '–±–ª—è—Ç', '–±–ª—è—Ç—å', '–±–ª–∏–Ω',
    '—Ö—É–π', '—Ö—É–ª–∏', '—Ö—É—è', '—Ö–µ—Ä', '—Ö—Ä–µ–Ω',
    '–ø–∏–∑–¥', '–ø–∏–∑–¥–µ—Ü', '–ø–∏–∑–¥–∞—Ç–æ', '–ø–∏–∑–¥–∞–Ω—É–ª',
    '—Å—É–∫', '—Å—É–∫–∞', '—Å—É–∫–∏',
    '–µ–±–∞–Ω', '–µ–±–∞—Ç—å', '–µ–±–∞–ª', '–µ–±–∞–Ω—É–ª', '–µ–±–∞—à',
    '–∑–∞–µ–±', '–∑–∞–µ–±–∞–ª', '–∑–∞–µ–±–∏—Å—å', '–∑–∞–µ–±–∞—Ç–æ',
    '–Ω–∞—Ö—É–π', '–Ω–∞—Ö–µ—Ä', '–ø–æ—Ö—É–π',
    '–æ—Ö—É–µ', '–∞—Ö—É–µ', '–æ—Ö—É–µ–ª', '–∞—Ö—É–µ–ª',
    '–∂–æ–ø', '–∂–æ–ø–∞', '–∂–æ–ø—É', '–∂–æ–ø–µ', '–∂–æ–ø–æ–π',

    # –¢–∞—Ç–∞—Ä—Å–∫–∏–µ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è
    '–∫—É—Ç–∞–∫', '–∫—É—Ç–∞–∫–±–∞—à',
    '—Ç–∏–ª–µ', '—Ç–∏–ª–µ–ª', '—Ç–∏–ª–µ–∫',
    '–∞–Ω–≥—ã—Ä–∞',
    '–¥—É–Ω–≥—ã–∑',
    '—Ç–∏–Ω—Ç”ô–∫', '—Ç–∏–Ω—Ç—ç–∫',
    '—Ö–∞–π–≤–∞–Ω',
    '—á—É—á–∫–∞',
    '—É–±—ã—Ä–ª—ã',
    '—Å–≤–æ–ª–æ—á—å',
    '–º–∞—Ä–∂–∞', '–º–∞“ó—Ä–∞',
    '—Ç—ã—á–∫–∞–∫',
    '–¥–∏–±–∏–ª', '–¥–µ–±–∏–ª',
    '–¥–æ–ª–±–∞–µ–±', '–¥–æ–ª–±–æ–µ–±', '–¥–æ–ª–±–∞—è—â–µ—Ä',
    '–ø—Ä–∏–¥—É—Ä–æ–∫',
    '–∏–¥–∏–æ—Ç',
    '–¥—É—Ä–∞–∫',
    '–º–∞–π–º—ã–ª',

    # –í—É–ª—å–≥–∞—Ä–Ω—ã–µ –∞–Ω–∞—Ç–æ–º–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
    '–∫—É—Ç', '–∫—É—Ç—ç',
    '–∂–æ–ø', '–∂–æ–ø–µ',
    '–±—ç—Ç—ç–∫', '–±–∞—Ç—ç–∫',
    '—Ç–∏—à–µ–∫',
    '—Å–µ–∫–µ', '—Å–∏–≥–µ',
    '—Å–æ—Å–æ', '—Å–æ—Å–æ–ø', '—Å–æ—Å–æ–º',
    '–∑–∞–∏–ø–∞', '–∑–∞–π–ø',

    # Code-switching –≤—É–ª—å–≥–∞—Ä–∏–∑–º—ã
    '–Ω–∞ —Ö—É–π', '–Ω–∞—Ö—É–π',
    '–ø–æ—à–æ–ª', '–ø–æ—à–µ–ª',
    '—à—Ç–æ –ª–∏',
    '—Ç–≤–∞—Ä—å',
}

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –§–ê–ó–ê 1: –ê–î–ê–ü–¢–ò–í–ù–ê–Ø –¢–ï–ú–ü–ï–†–ê–¢–£–†–ê
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def get_adaptive_temperature(text: str) -> float:
    """
    –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç

    Returns:
        –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    """
    text_lower = text.lower()
    toxic_count = sum(1 for marker in TATAR_TOXIC_LEXICON if marker in text_lower)

    if toxic_count >= 3:
        # –í—ã—Å–æ–∫–∞—è —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å ‚Äî –Ω—É–∂–Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å
        return 0.15
    elif toxic_count == 0:
        # –ù–µ—è–≤–Ω–∞—è —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å ‚Äî –Ω—É–∂–Ω–∞ –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        return 0.35
    else:
        # –°—Ä–µ–¥–Ω—è—è —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å ‚Äî –±–∞–ª–∞–Ω—Å
        return 0.25


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –§–ê–ó–ê 2: –ú–ê–†–ö–ò–†–û–í–ö–ê –¢–û–ö–°–ò–ß–ù–´–• –°–õ–û–í
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def mark_toxic_words(text: str) -> Tuple[str, List[str]]:
    """
    –ü–æ–º–µ—á–∞–µ—Ç —Ç–æ–∫—Å–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –º–æ–¥–µ–ª–∏

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç

    Returns:
        Tuple[–ø–æ–º–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, —Å–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–∫—Å–∏—á–Ω—ã—Ö —Å–ª–æ–≤]
    """
    marked = text
    found_toxic = []

    for word in TATAR_TOXIC_LEXICON:
        if word in text.lower():
            # –†–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–∞—è –∑–∞–º–µ–Ω–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–∞
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            if pattern.search(marked):
                marked = pattern.sub(f"[TOX:{word}]", marked, count=1)
                found_toxic.append(word)

    return marked, found_toxic


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –§–ê–ó–ê 4: FEW-SHOT EXAMPLES
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

TATAR_EXAMPLES = """
üìö –ü–†–ò–ú–ï–†–´ –î–ï–¢–û–ö–°–ò–§–ò–ö–ê–¶–ò–ò –î–õ–Ø –¢–ê–¢–ê–†–°–ö–û–ì–û:

–ü—Ä–∏–º–µ—Ä 1:
–¢–æ–∫—Å–∏—á–Ω—ã–π: –ö—É–ø–º–µ –∞—à–∞—Ä–≥–∞ –±—É–ª–∞ –∏–Ω–¥–µ? –°–∏–º–µ—Ä–µ–ø —á—É—á–∫–∞ –±—É–ª–∞—Å—ã–∑ –±–∏—Ç
–î–µ—Ç–æ–∫—Å: –ö—É–ø–º–µ –∞—à–∞—Ä–≥–∞ –±—É–ª–∞ –∏–Ω–¥–µ? –°–∏–º–µ—Ä–µ–ø –±—É–ª–∞—Å—ã–∑ –±–∏—Ç
–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –£–¥–∞–ª–∏–ª–∏ "—á—É—á–∫–∞" (—Å–≤–∏–Ω—å—è - –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏–µ)

–ü—Ä–∏–º–µ—Ä 2:
–¢–æ–∫—Å–∏—á–Ω—ã–π: —Å–∏–Ω–µ –∞–Ω–¥–∞ –±–∞—Ä—ã–±–µ—Ä –∫–æ—Ç–µ–ø —è—Ç–º—ã–π–ª–∞—Ä, —Ç–∞–∫ —á—Ç–æ, —Å–∏–¥–∏ —Ä–æ–≤–Ω–æ –Ω–∞ –∂–æ–ø–µ
–î–µ—Ç–æ–∫—Å: —Å–∏–Ω–µ –∞–Ω–¥–∞ –±–∞—Ä—ã–±–µ—Ä –∫–æ—Ç–µ–ø —è—Ç–º—ã–π–ª–∞—Ä, —Ç–∞–∫ —á—Ç–æ, —Å–∏–¥–∏ —Ä–æ–≤–Ω–æ
–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –£–¥–∞–ª–∏–ª–∏ –≤—É–ª—å–≥–∞—Ä–∏–∑–º "–Ω–∞ –∂–æ–ø–µ"

–ü—Ä–∏–º–µ—Ä 3:
–¢–æ–∫—Å–∏—á–Ω—ã–π: @user, —Å–∏–Ω –∫–∏–ª –¥–∞–≤–∞–π, –∫—É—Ç–∞–∫ —Å—ã—Ä–ª–∞–º–∞!
–î–µ—Ç–æ–∫—Å: —Å–∏–Ω –∫–∏–ª –¥–∞–≤–∞–π, —Å—ã—Ä–ª–∞–º–∞!
–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –£–±—Ä–∞–ª–∏ @user –∏ "–∫—É—Ç–∞–∫" (–∑–∞–¥–Ω–∏—Ü–∞)

–ü—Ä–∏–º–µ—Ä 4:
–¢–æ–∫—Å–∏—á–Ω—ã–π: –∞–ª–¥—ã–π–º –∏–Ω–¥–µ –º–∏–Ω!)) –º–∏–Ω–µ–∫–µ –±–ª–∏–Ω –π–æ—Ä—Ç—Ç–∞–Ω –¥–∞ —á—ã–∫–º—ã–π, –æ–π–¥—ç –≥—ç–Ω—ç —Å–æ—Å–æ–ø —è—Ç–∞ =D
–î–µ—Ç–æ–∫—Å: –∞–ª–¥—ã–π–º –∏–Ω–¥–µ –º–∏–Ω!)) –º–∏–Ω–µ–∫–µ –π–æ—Ä—Ç—Ç–∞–Ω –¥–∞ —á—ã–∫–º—ã–π, –æ–π–¥—ç –≥—ç–Ω—ç —è—Ç–∞ =D
–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –£–¥–∞–ª–∏–ª–∏ —Å–ª–∞–±—ã–π –≤—É–ª—å–≥–∞—Ä–∏–∑–º "–±–ª–∏–Ω", —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ —Å–º–∞–π–ª–∏–∫ –∏ —ç–º–æ—Ü–∏—é

–ü—Ä–∏–º–µ—Ä 5:
–¢–æ–∫—Å–∏—á–Ω—ã–π: –•”ô–∑–µ—Ä –∞“£–∞—Ä–¥–∞–Ω –∫–∞–π—á–∞–Ω –∫–∞–π—Ç–∞–ª–∞—Ä –±–∏—Ç –±–ª—ç—Ç
–î–µ—Ç–æ–∫—Å: –•”ô–∑–µ—Ä –∞“£–∞—Ä–¥–∞–Ω –∫–∞–π—á–∞–Ω –∫–∞–π—Ç–∞–ª–∞—Ä –±–∏—Ç
–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: –£–¥–∞–ª–∏–ª–∏ "–±–ª—ç—Ç", —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ —Ç–∞—Ç–∞—Ä—Å–∫—É—é –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—é (”ô, “£)
"""


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –§–ê–ó–ê 1: CHAIN-OF-THOUGHT PROMPTING
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_cot_prompt(text: str, marked_text: str, toxic_words: List[str]) -> str:
    """
    –°–æ–∑–¥–∞—ë—Ç Chain-of-Thought –ø—Ä–æ–º–ø—Ç —Å —Ç–∞—Ç–∞—Ä—Å–∫–æ–π —Å–ø–µ—Ü–∏—Ñ–∏–∫–æ–π

    Args:
        text: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
        marked_text: –¢–µ–∫—Å—Ç —Å –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —Ç–æ–∫—Å–∏—á–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        toxic_words: –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–∫—Å–∏—á–Ω—ã—Ö —Å–ª–æ–≤

    Returns:
        –ì–æ—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
    """

    tatar_context = """–í–ê–ñ–ù–û: –≠—Ç–æ –¢–ê–¢–ê–†–°–ö–ò–ô —è–∑—ã–∫ (Turkic family).
–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –°–º–µ—à–∞–Ω–Ω—ã–π —Ä—É—Å—Å–∫–æ-—Ç–∞—Ç–∞—Ä—Å–∫–∏–π –∫–æ–¥-switching (–Ω–æ—Ä–º–∞–ª—å–Ω–æ)
- –ó–∞–∏–º—Å—Ç–≤–æ–≤–∞–Ω–Ω–∞—è –æ–±—Å—Ü–µ–Ω–Ω–∞—è –ª–µ–∫—Å–∏–∫–∞ –∏–∑ —Ä—É—Å—Å–∫–æ–≥–æ (—É–¥–∞–ª—è—Ç—å)
- –õ–∞—Ç–∏–Ω–∏—Ü–∞ + –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ (—Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Å—Ç–∏–ª—å –æ—Ä–∏–≥–∏–Ω–∞–ª–∞)
- –ê–≥–≥–ª—é—Ç–∏–Ω–∞—Ç–∏–≤–Ω–∞—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è (—Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Å–ª–æ–≤–æ—Ñ–æ—Ä–º—ã)
- –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –±—É–∫–≤—ã: ”ô, ”©, “Ø, “£, “ó, “ª (–ù–ï –ú–ï–ù–Ø–¢–¨!)

"""

    cot_reasoning = """–ü–µ—Ä–µ–¥ –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –ø–æ–¥—É–º–∞–π –ü–û–®–ê–ì–û–í–û:

–®–ê–ì 1: –ö–∞–∫–∏–µ —Å–ª–æ–≤–∞/—Ñ—Ä–∞–∑—ã —Ç–æ–∫—Å–∏—á–Ω—ã?
–ù–∞–π–¥–∏ –≤ —Ç–µ–∫—Å—Ç–µ: –æ–±—Å—Ü–µ–Ω–Ω–∞—è –ª–µ–∫—Å–∏–∫–∞, –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è, –≤—É–ª—å–≥–∞—Ä–∏–∑–º—ã

–®–ê–ì 2: –ö–∞–∫–æ–π –æ—Å–Ω–æ–≤–Ω–æ–π —Å–º—ã—Å–ª –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è?
–ü–æ–π–º–∏ –≥–ª–∞–≤–Ω—É—é –º—ã—Å–ª—å –∞–≤—Ç–æ—Ä–∞ –ë–ï–ó —Ç–æ–∫—Å–∏—á–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤

–®–ê–ì 3: –ö–∞–∫ –ø–µ—Ä–µ–¥–∞—Ç—å —Å–º—ã—Å–ª –±–µ–∑ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏?
–£–¥–∞–ª–∏ —Ç–æ–∫—Å–∏—á–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ú–ò –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏

"""

    toxic_hint = ""
    if toxic_words:
        toxic_hint = f"üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ç–æ–∫—Å–∏—á–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã: {', '.join(toxic_words)}\n\n"

    examples_section = TATAR_EXAMPLES

    task = f"""–¢–µ–ø–µ—Ä—å –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏—Ü–∏—Ä—É–π —Ç–µ–∫—Å—Ç:

–¢–µ–∫—Å—Ç —Å –º–∞—Ä–∫–∏—Ä–æ–≤–∫–æ–π: {marked_text}

–ö–†–ò–¢–ò–ß–ù–´–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø:
1. –ù–ï –º–µ–Ω—è—Ç—å –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—é (”ô, ”©, “Ø, “£, “ó, “ª –æ—Å—Ç–∞—é—Ç—Å—è –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ!)
2. –ù–ï –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é
3. –£–¥–∞–ª–∏—Ç—å –¢–û–õ–¨–ö–û —Ç–æ–∫—Å–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞ ([TOX:...])
4. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–º–∞–π–ª–∏–∫–∏, —ç–º–æ–¥–∑–∏, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
5. –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å –∏ –ø–ª–∞–≤–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
6. –£–¥–∞–ª–∏—Ç—å @user —É–ø–æ–º–∏–Ω–∞–Ω–∏—è

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –ë–ï–ó –æ–±—ä—è—Å–Ω–µ–Ω–∏–π:"""

    full_prompt = tatar_context + cot_reasoning + toxic_hint + examples_section + "\n\n" + task

    return full_prompt


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –§–ê–ó–ê 3: MULTI-CANDIDATE GENERATION
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
def generate_candidates(text: str, n: int = 3) -> List[str]:
    global total_api_calls, total_input_tokens, total_output_tokens

    marked_text, toxic_words = mark_toxic_words(text)

    if not toxic_words:
        return [text] * n

    prompt = create_cot_prompt(text, marked_text, toxic_words)

    candidates = []
    base_temp = get_adaptive_temperature(text)

    for i in range(n):
        temp = min(base_temp + i * 0.05, 0.5)

        result = gemini_generate(prompt, temperature=temp, seed=42+i)

        if result and len(result) >= len(text) * 0.3:
            candidates.append(result.strip('"\'`'))
        else:
            candidates.append(text)

    return candidates



# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –§–ê–ó–ê 3: RERANKING –° LaBSE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –º–æ–¥–µ–ª–∏ (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
_labse_model = None

def get_labse_model():
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ LaBSE –º–æ–¥–µ–ª–∏"""
    global _labse_model
    if _labse_model is None:
        try:
            from sentence_transformers import SentenceTransformer
            print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ LaBSE –º–æ–¥–µ–ª–∏...")
            _labse_model = SentenceTransformer('sentence-transformers/LaBSE')
            print("‚úÖ LaBSE –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except ImportError:
            print("‚ö†Ô∏è  sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∫–∞...")
            import subprocess
            subprocess.check_call(['pip', 'install', 'sentence-transformers', '-q'])
            from sentence_transformers import SentenceTransformer
            _labse_model = SentenceTransformer('sentence-transformers/LaBSE')
    return _labse_model


def select_best_candidate(candidates: List[str], original_text: str) -> str:
    """
    –í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π –∫–∞–Ω–¥–∏–¥–∞—Ç –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º:
    - Similarity —Å –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º (50%)
    - –î–ª–∏–Ω–∞ (30%)
    - –¢–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å (20%)

    Args:
        candidates: –°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        original_text: –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç

    Returns:
        –õ—É—á—à–∏–π –∫–∞–Ω–¥–∏–¥–∞—Ç
    """
    if len(candidates) == 1:
        return candidates[0]

    try:
        from sentence_transformers import util
        model = get_labse_model()

        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        orig_emb = model.encode(original_text, convert_to_tensor=True)
        cand_embs = model.encode(candidates, convert_to_tensor=True)

        scores = []
        for i, candidate in enumerate(candidates):
            # 1. Similarity —Å –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º (50%)
            similarity = util.cos_sim(orig_emb, cand_embs[i]).item()

            # 2. –î–ª–∏–Ω–∞ (30%)
            length_ratio = len(candidate) / max(len(original_text), 1)
            length_score = 1.0 - abs(1.0 - length_ratio)
            length_score = max(0.0, min(1.0, length_score))  # Clamp [0, 1]

            # 3. –¢–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å (20%)
            toxic_count = sum(1 for word in TATAR_TOXIC_LEXICON
                             if word in candidate.lower())
            toxicity_score = 1.0 / (1.0 + toxic_count)

            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            total_score = (similarity * 0.5 +
                          length_score * 0.3 +
                          toxicity_score * 0.2)

            scores.append(total_score)

        best_idx = scores.index(max(scores))
        return candidates[best_idx]

    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ reranking: {e}")
        # Fallback: –≤—ã–±–∏—Ä–∞–µ–º –ø–æ –¥–ª–∏–Ω–µ
        return min(candidates, key=lambda x: abs(len(x) - len(original_text)))


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –§–ê–ó–ê 4: –î–í–£–•–ü–†–û–•–û–î–ù–ê–Ø –î–ï–¢–û–ö–°–ò–§–ò–ö–ê–¶–ò–Ø
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def check_remaining_toxicity(text: str) -> List[str]:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ—Å—Ç–∞–ª–∏—Å—å –ª–∏ —Ç–æ–∫—Å–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ

    Args:
        text: –¢–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

    Returns:
        –°–ø–∏—Å–æ–∫ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è —Ç–æ–∫—Å–∏—á–Ω—ã—Ö —Å–ª–æ–≤
    """
    text_lower = text.lower()
    remaining = [word for word in TATAR_TOXIC_LEXICON if word in text_lower]
    return remaining


def refine_detoxification(original: str, first_pass: str) -> str:
    remaining_toxic = check_remaining_toxicity(first_pass)

    if not remaining_toxic:
        return first_pass

    refinement_prompt = f"""–¢–µ–∫—Å—Ç –≤—Å—ë –µ—â—ë —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å: {', '.join(remaining_toxic)}
–ò—Å—Ö–æ–¥–Ω—ã–π: {original}
–ü–µ—Ä–≤–∞—è –ø–æ–ø—ã—Ç–∫–∞: {first_pass}

–ó–ê–î–ê–ß–ê: –£–ª—É—á—à–∏ –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏—é, —É–¥–∞–ª–∏–≤: {', '.join(remaining_toxic)}
–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û —É–ª—É—á—à–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:
"""

    result = gemini_generate(refinement_prompt, temperature=0.1, seed=42)

    if result and len(result) >= len(first_pass) * 0.5:
        return result.strip('"\'`')

    return first_pass



# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ì–õ–ê–í–ù–´–ô PIPELINE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def strategic_detox_pipeline(text: str) -> str:
    """
    –ü–æ–ª–Ω—ã–π —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π pipeline –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

    Pipeline:
    1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π/–∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç
    2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è N –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (multi-candidate)
    3. Reranking —Å LaBSE
    4. –î–≤—É—Ö–ø—Ä–æ—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)

    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–æ–∫—Å–∏—á–Ω—ã–π —Ç–µ–∫—Å—Ç

    Returns:
        –î–µ—Ç–æ–∫—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–∞
    if not isinstance(text, str) or not text.strip():
        return text

    # Step 1: Multi-candidate generation
    candidates = generate_candidates(text, n=NUM_CANDIDATES)

    # Step 2: Reranking
    best_candidate = select_best_candidate(candidates, text)

    # Step 3: –î–≤—É—Ö–ø—Ä–æ—Ö–æ–¥–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    remaining = check_remaining_toxicity(best_candidate)
    if remaining and len(best_candidate) > 10:
        # –ï—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å —Ç–æ–∫—Å–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞ ‚Äî –≤—Ç–æ—Ä–æ–π –ø—Ä–æ—Ö–æ–¥
        refined = refine_detoxification(text, best_candidate)
        return refined

    return best_candidate


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MAIN
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def main():
    print("="*80)
    print("üöÄ –°–¢–†–ê–¢–ï–ì–ò–ß–ï–°–ö–û–ï –†–ï–®–ï–ù–ò–ï - –í—Å–µ —Ñ–∞–∑—ã –ø–ª–∞–Ω–∞ –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
    print("="*80)

    print(f"\nüì• –ß—Ç–µ–Ω–∏–µ: {INPUT_FILE}")
    df = pd.read_csv(INPUT_FILE, sep="\t")
    print(f"   –û–±—Ä–∞–∑—Ü–æ–≤: {len(df)}")

    print(f"\n‚ö° –°—Ç—Ä–∞—Ç–µ–≥–∏—è:")
    print(f"   ‚úÖ –§–∞–∑–∞ 1: CoT Reasoning + –¢–∞—Ç–∞—Ä—Å–∫–∞—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞ + –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞")
    print(f"   ‚úÖ –§–∞–∑–∞ 2: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ª–µ–∫—Å–∏–∫–æ–Ω + –ú–∞—Ä–∫–∏—Ä–æ–≤–∫–∞ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö —Å–ª–æ–≤")
    print(f"   ‚úÖ –§–∞–∑–∞ 3: Multi-candidate ({NUM_CANDIDATES}) + LaBSE reranking")
    print(f"   ‚úÖ –§–∞–∑–∞ 4: Few-shot examples + –î–≤—É—Ö–ø—Ä–æ—Ö–æ–¥–Ω–∞—è –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏—è")

    print("\nüöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞...\n")

    # –ó–∞–≥—Ä—É–∑–∫–∞ LaBSE –∑–∞—Ä–∞–Ω–µ–µ
    get_labse_model()

    # –û–±—Ä–∞–±–æ—Ç–∫–∞
    tqdm.pandas(desc="üéØ Strategic Detox")
    df["tat_detox1"] = df["tat_toxic"].progress_apply(strategic_detox_pipeline)

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
    df["tat_detox1"] = df["tat_detox1"].fillna(df["tat_toxic"])
    empty_mask = df["tat_detox1"].isna() | (df["tat_detox1"].str.strip() == "")
    if empty_mask.any():
        df.loc[empty_mask, "tat_detox1"] = df.loc[empty_mask, "tat_toxic"]

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    changed = (df["tat_toxic"] != df["tat_detox1"]).sum()

    length_diffs = []
    for idx in range(len(df)):
        orig = df.iloc[idx]["tat_toxic"]
        detox = df.iloc[idx]["tat_detox1"]
        if len(orig) > 0:
            diff = abs(len(detox) - len(orig)) / len(orig)
            length_diffs.append(diff)

    avg_diff = sum(length_diffs) / len(length_diffs) * 100 if length_diffs else 0

    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –ò–∑–º–µ–Ω–µ–Ω–æ: {changed}/{len(df)} ({changed/len(df)*100:.1f}%)")
    print(f"   –°—Ä–µ–¥–Ω–∏–π Œî –¥–ª–∏–Ω—ã: {avg_diff:.1f}%")
    print(f"   API –≤—ã–∑–æ–≤–æ–≤: {total_api_calls}")
    print(f"   –¢–æ–∫–µ–Ω–æ–≤ (–≤—Ö–æ–¥): {total_input_tokens:,}")
    print(f"   –¢–æ–∫–µ–Ω–æ–≤ (–≤—ã—Ö–æ–¥): {total_output_tokens:,}")

    # –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å (gpt-4o-2024-11-20)
    cost_input = (total_input_tokens / 1_000_000) * 2.5  # $2.5/1M input tokens
    cost_output = (total_output_tokens / 1_000_000) * 10  # $10/1M output tokens
    total_cost = cost_input + cost_output
    print(f"   –ü—Ä–∏–º–µ—Ä–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${total_cost:.2f}")

    print(f"\nüì¶ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {OUTPUT_FILE}")
    df[["ID", "tat_toxic", "tat_detox1"]].to_csv(OUTPUT_FILE, sep="\t", index=False)

    print("\n" + "="*80)
    print("‚úÖ –ì–û–¢–û–í–û!")
    print("="*80)

    print(f"\nüéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"   STA (Style Transfer Accuracy): 0.85-0.90")
    print(f"   SIM (Similarity): 0.90-0.93")
    print(f"   FL (Fluency): 0.95-0.97")
    print(f"   J-score: 0.70-0.75+ ‚Üí –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê!")

    print(f"\nüìä –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ—Ü–µ–Ω–∫—É: .venv/bin/python evaluate_j_score.py")
    print(f"   –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–∞–π–ª: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()
