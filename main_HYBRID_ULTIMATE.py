#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üèÜ –ì–ò–ë–†–ò–î–ù–û–ï –†–ï–®–ï–ù–ò–ï - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è —Ö–∞–∫–∞—Ç–æ–Ω–∞

–°–¢–†–ê–¢–ï–ì–ò–Ø:
1. MT0-XL-DETOX-ORPO (—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å) - –±—ã—Å—Ç—Ä–∞—è –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
2. GPT-4o-mini (—Å —É—Å–∏–ª–µ–Ω–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º) - –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤
3. –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞

–û–ñ–ò–î–ê–ï–ú–´–ô J-SCORE: 0.72-0.78 üéØ

–ú–æ–¥–µ–ª–∏:
- s-nlp/mt0-xl-detox-orpo (3.7B, multilingual, ORPO-aligned)
- GPT-4o-mini (—Å Chain-of-Thought)
"""

import re
import os
import pandas as pd
from openai import OpenAI
from tqdm import tqdm
import time
import torch
from typing import List, Tuple, Optional
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ù–ê–°–¢–†–û–ô–ö–ò
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# GPT-4o-mini API
API_KEY = "sk-C4Ju9Yy2-EKOf6SHs-jBPA"
BASE_URL = "https://api.artemox.com/v1"
MODEL_NAME = "gpt-4o-mini"

client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

# –§–∞–π–ª—ã
INPUT_FILE = "dev_inputs.tsv"
OUTPUT_FILE = "submission_hybrid_ultimate.tsv"

# –†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã
USE_MT0 = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å mt0-xl-detox-orpo
USE_GPT = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPT-4o-mini
HYBRID_MODE = "ensemble"  # "mt0_only", "gpt_only", "ensemble"

# –°—á–µ—Ç—á–∏–∫–∏
total_api_calls = 0
stats = {
    'mt0_used': 0,
    'gpt_used': 0,
    'ensemble_used': 0
}

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –¢–û–ö–°–ò–ß–ù–´–ô –õ–ï–ö–°–ò–ö–û–ù
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

TATAR_TOXIC_LEXICON = {
    'explicit_russian': [
        '–±–ª—è', '–±–ª—è—Ç', '–±–ª—ç—Ç', '–±–ª—è—Ç—å',
        '—Ö—É–π', '—Ö—É—è', '—Ö—É—é', '—Ö—É–µ–º', '—Ö—É–ª–∏', '—Ö—É–π–Ω—è',
        '–ø–∏–∑–¥', '–ø–∏–∑–¥–µ', '–ø–∏–∑–¥–µ—Ü', '–ø–∏–∑–¥—É',
        '–µ–±–∞–Ω', '–µ–±–∞—Ç—å', '–µ–±–∞–ª', '–µ–±–ª–æ', '–µ–±–∞—à',
        '—Å—É–∫–∞', '—Å—É–∫–∏', '—Å—É–∫',
        '–ø–∏–¥–æ—Ä', '–ø–∏–¥–∞—Ä', '–ø–∏–¥—Ä',
    ],
    'vulgar_russian': [
        '–∂–æ–ø', '–∂–æ–ø–∞', '–∂–æ–ø–µ', '–∂–æ–ø—É', '–∂–æ–ø–æ–π',
        '—Å—Ä–∞—Ç—å', '—Å—Ä–∞–∫–∞', '—Å—Ä–∞–∫—É',
        '–≥–∞–≤–Ω–æ', '–≥–æ–≤–Ω–æ', '–≥–æ–≤–Ω–∞',
        '–¥–µ—Ä—å–º–æ', '–¥–µ—Ä—å–º–∞',
    ],
    'weak_russian': [
        '–±–ª–∏–Ω', '–±–ª–∏–Ω—Å–∫–∏–π',
        '—Ö—Ä–µ–Ω', '—Ö—Ä–µ–Ω–∞', '—Ö—Ä–µ–Ω–æ–≤',
        '—á–µ—Ä—Ç', '—á–µ—Ä—Ç–∞', '—á–µ—Ä—Ç–æ–≤',
        '—Ñ–∏–≥', '—Ñ–∏–≥–∞',
    ],
    'explicit_tatar': [
        '–∫—É—Ç–∞–∫', '–∫—É—Ç–∞–∫–±–∞—à', '–∫—É—Ç–∞–∫–ª–∞—Ä',
        '—Ç–∏–ª–µ', '—Ç–∏–ª–µ–¥–µ—Ä',
        '–¥—É–Ω–≥—ã–∑', '—á—É—á–∫–∞',
        '—Ç–∏–Ω—Ç”ô–∫', '—Ç–∏–Ω—Ç–µ–∫–ª”ô—Ä', '—Ç–∏–Ω—Ç—ç–∫',
        '–∞–Ω–≥—ã—Ä–∞', '–∞–Ω–≥—ã—Ä–∞–ª—ã',
        '—É–±—ã—Ä–ª—ã', '—É–±—ã—Ä–ª—ã–∫',
    ],
    'vulgar_tatar': [
        '—Å–æ—Å–æ–ø', '—Å–æ—Å—É',
        '—Ç—ã—á–∫–∞–∫', '—Ç—ã—á–∫–∞–∫–ª–∞—Ä',
        '–º–∞—Ä–∂–∞', '–º–∞“ó—Ä–∞',
        '–±—ç—Ç—ç–∫', '—Ç–∏—à–µ–∫',
    ],
    'code_switching': [
        '–Ω–∞ —Ö—É–π', '–Ω–∞—Ö—É–π', '–Ω–∞ —Ö–µ—Ä',
        '–ø–æ—à–æ–ª', '–∏–¥–∏ –Ω–∞',
        '—à—Ç–æ –ª–∏',
    ],
}

def get_all_toxic_words() -> set:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–ª–æ—Å–∫–∏–π set –≤—Å–µ—Ö —Ç–æ–∫—Å–∏—á–Ω—ã—Ö —Å–ª–æ–≤"""
    all_words = []
    for category in TATAR_TOXIC_LEXICON.values():
        all_words.extend(category)
    return set(all_words)

TOXIC_WORDS_SET = get_all_toxic_words()

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MT0-XL-DETOX-ORPO –ú–û–î–ï–õ–¨
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

_mt0_model = None
_mt0_tokenizer = None

def load_mt0_model():
    """–õ–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ mt0-xl-detox-orpo –º–æ–¥–µ–ª–∏"""
    global _mt0_model, _mt0_tokenizer

    if _mt0_model is None:
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ mt0-xl-detox-orpo –º–æ–¥–µ–ª–∏...")
        try:
            device = "cuda" if torch.cuda.is_available() else "cpu"

            _mt0_tokenizer = AutoTokenizer.from_pretrained('s-nlp/mt0-xl-detox-orpo')
            _mt0_model = AutoModelForSeq2SeqLM.from_pretrained(
                's-nlp/mt0-xl-detox-orpo',
                device_map="auto" if torch.cuda.is_available() else None,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            )

            if not torch.cuda.is_available():
                _mt0_model = _mt0_model.to(device)

            _mt0_model.eval()
            print(f"‚úÖ MT0 –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device}")
            print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: 3.7B")

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ MT0: {e}")
            return None, None

    return _mt0_model, _mt0_tokenizer

def detoxify_with_mt0(text: str, num_beams: int = 5) -> str:
    """
    –î–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é mt0-xl-detox-orpo

    Args:
        text: –¢–æ–∫—Å–∏—á–Ω—ã–π —Ç–µ–∫—Å—Ç
        num_beams: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ beam search –ª—É—á–µ–π

    Returns:
        –î–µ—Ç–æ–∫—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    model, tokenizer = load_mt0_model()

    if model is None or tokenizer is None:
        return text

    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä—É—Å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç (—Ç–∞—Ç–∞—Ä—Å–∫–∏–π –±–ª–∏–∑–æ–∫ –∫ —Ä—É—Å—Å–∫–æ–º—É –≤ —ç—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ)
        prompt = f"–î–µ—Ç–æ–∫—Å–∏—Ñ–∏—Ü–∏—Ä—É–π: {text}"

        inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=512)

        # –ü–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                num_beams=num_beams,
                temperature=0.7,
                do_sample=False,  # Deterministic with beam search
                early_stopping=True
            )

        detoxed = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # –û—á–∏—Å—Ç–∫–∞
        detoxed = detoxed.strip()

        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –µ–≥–æ –ø–æ–≤—Ç–æ—Ä–∏–ª–∞
        if detoxed.startswith("–î–µ—Ç–æ–∫—Å–∏—Ñ–∏—Ü–∏—Ä—É–π:"):
            detoxed = detoxed[len("–î–µ—Ç–æ–∫—Å–∏—Ñ–∏—Ü–∏—Ä—É–π:"):].strip()

        return detoxed if detoxed else text

    except Exception as e:
        print(f"‚ö†Ô∏è MT0 detox error: {e}")
        return text

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# GPT-4o-mini –î–ï–¢–û–ö–°–ò–§–ò–ö–ê–¶–ò–Ø
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_gpt_prompt(text: str) -> str:
    """–ö—Ä–∞—Ç–∫–∏–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è GPT-4o-mini"""

    prompt = f"""–î–µ—Ç–æ–∫—Å–∏—Ñ–∏—Ü–∏—Ä—É–π —Ç–∞—Ç–∞—Ä—Å–∫–∏–π —Ç–µ–∫—Å—Ç, —É–¥–∞–ª–∏–≤ –¢–û–õ–¨–ö–û —Ç–æ–∫—Å–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞.

–¢–û–ö–°–ò–ß–ù–û–°–¢–¨ (—É–¥–∞–ª–∏—Ç—å):
- –†—É—Å—Å–∫–∏–µ –º–∞—Ç—ã: –±–ª—è, —Ö—É–π, –ø–∏–∑–¥, –µ–±–∞–Ω, —Å—É–∫–∞, –∂–æ–ø–∞
- –¢–∞—Ç–∞—Ä—Å–∫–∏–µ –æ—Å–∫–æ—Ä–±–ª–µ–Ω–∏—è: –∫—É—Ç–∞–∫, —Ç–∏–ª–µ, –¥—É–Ω–≥—ã–∑, —á—É—á–∫–∞, —Ç–∏–Ω—Ç”ô–∫, –∞–Ω–≥—ã—Ä–∞
- –í—É–ª—å–≥–∞—Ä–∏–∑–º—ã: –±–ª–∏–Ω, —Ö—Ä–µ–Ω, —Å–æ—Å–æ–ø, —Ç—ã—á–∫–∞–∫
- @—É–ø–æ–º–∏–Ω–∞–Ω–∏—è

–í–ê–ñ–ù–û:
‚úì –£–¥–∞–ª–∏ –¢–û–õ–¨–ö–û —Ç–æ–∫—Å–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞
‚úì –°–æ—Ö—Ä–∞–Ω–∏ —Å–º—ã—Å–ª, –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—é, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
‚úì –í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç

–¢–µ–∫—Å—Ç: {text}"""

    return prompt

def detoxify_with_gpt(text: str) -> str:
    """–î–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é GPT-4o-mini"""
    global total_api_calls

    try:
        prompt = create_gpt_prompt(text)

        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=300,
            seed=42
        )

        total_api_calls += 1

        detoxed = resp.choices[0].message.content.strip()
        detoxed = detoxed.strip('"\'`')

        # –£–¥–∞–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å—ã
        prefixes = ['–¥–µ—Ç–æ–∫—Å:', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç:', '–æ—Ç–≤–µ—Ç:']
        detoxed_lower = detoxed.lower()
        for prefix in prefixes:
            if detoxed_lower.startswith(prefix):
                detoxed = detoxed[len(prefix):].strip()
                break

        return detoxed if detoxed else text

    except Exception as e:
        print(f"‚ö†Ô∏è GPT detox error: {e}")
        time.sleep(1)
        return text

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–û–ï –†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–ï
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def check_toxicity(text: str) -> int:
    """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö —Å–ª–æ–≤"""
    text_lower = text.lower()
    return sum(1 for word in TOXIC_WORDS_SET if word in text_lower)

def calculate_candidate_score(candidate: str, original: str) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç –æ—Ü–µ–Ω–∫—É –∫–∞–Ω–¥–∏–¥–∞—Ç–∞

    –ö—Ä–∏—Ç–µ—Ä–∏–∏:
    - –î–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (45%): –º–µ–Ω—å—à–µ —Ç–æ–∫—Å–∏—á–Ω—ã—Ö —Å–ª–æ–≤ = –ª—É—á—à–µ
    - Similarity (35%): –ø–æ—Ö–æ–∂–µ—Å—Ç—å –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª
    - Fluency (20%): –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
    """
    score = 0.0

    # 1. –î–ï–¢–û–ö–°–ò–§–ò–ö–ê–¶–ò–Ø (45%)
    toxic_count = check_toxicity(candidate)
    detox_score = 1.0 / (1.0 + toxic_count)
    score += detox_score * 0.45

    # 2. SIMILARITY (35%)
    orig_words = set(original.lower().split())
    cand_words = set(candidate.lower().split())

    if orig_words:
        jaccard = len(orig_words & cand_words) / len(orig_words | cand_words)
    else:
        jaccard = 1.0

    length_ratio = min(len(candidate), len(original)) / max(len(candidate), len(original), 1)
    similarity = (jaccard * 0.7 + length_ratio * 0.3)
    score += similarity * 0.35

    # 3. FLUENCY (20%)
    fluency = 1.0

    words = candidate.strip().split()
    if words:
        last_word = words[-1].lower()
        if last_word in ['–Ω–∞', '–≤', '—Å', '–∫', '–ø–æ', '–∑–∞', '–∏', '–∞', '–Ω–æ', '–¥–∞', '–ª–∏']:
            fluency *= 0.5

    if len(words) < 3:
        fluency *= 0.7

    if not candidate.strip():
        fluency = 0.0

    score += fluency * 0.20

    return score

def select_best_result(mt0_result: str, gpt_result: str, original: str) -> Tuple[str, str]:
    """
    –í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ MT0 –∏ GPT

    Returns:
        (best_result, source: 'mt0' –∏–ª–∏ 'gpt' –∏–ª–∏ 'ensemble')
    """
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏
    mt0_toxic = check_toxicity(mt0_result)
    gpt_toxic = check_toxicity(gpt_result)
    orig_toxic = check_toxicity(original)

    # –ï—Å–ª–∏ –æ–¥–∏–Ω –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—á–∏—â–µ–Ω, –∞ –¥—Ä—É–≥–æ–π –Ω–µ—Ç
    if mt0_toxic == 0 and gpt_toxic > 0:
        return mt0_result, 'mt0'
    if gpt_toxic == 0 and mt0_toxic > 0:
        return gpt_result, 'gpt'

    # –ï—Å–ª–∏ –æ–±–∞ –Ω–µ –æ—á–∏—Å—Ç–∏–ª–∏ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª (–Ω–µ—Ç —è–≤–Ω–æ–π —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏)
    if mt0_toxic == orig_toxic and gpt_toxic == orig_toxic:
        return original, 'original'

    # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–µ
    mt0_score = calculate_candidate_score(mt0_result, original)
    gpt_score = calculate_candidate_score(gpt_result, original)

    if mt0_score > gpt_score:
        return mt0_result, 'mt0'
    else:
        return gpt_result, 'gpt'

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–ï–¢–û–ö–°–ò–§–ò–ö–ê–¶–ò–ò
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def hybrid_detoxify(text: str) -> str:
    """
    –ì–∏–±—Ä–∏–¥–Ω–∞—è –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º –≤—ã–±–æ—Ä–æ–º

    Args:
        text: –¢–æ–∫—Å–∏—á–Ω—ã–π —Ç–µ–∫—Å—Ç

    Returns:
        –î–µ—Ç–æ–∫—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    global stats

    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    if not isinstance(text, str) or not text.strip():
        return text

    # –ü—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å—Ç—å –ª–∏ —è–≤–Ω–∞—è —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å?
    orig_toxic_count = check_toxicity(text)

    # –ï—Å–ª–∏ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –Ω–µ—Ç - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
    if orig_toxic_count == 0:
        return text

    # –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã
    if HYBRID_MODE == "mt0_only":
        stats['mt0_used'] += 1
        return detoxify_with_mt0(text)

    elif HYBRID_MODE == "gpt_only":
        stats['gpt_used'] += 1
        return detoxify_with_gpt(text)

    else:  # ensemble
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π
        mt0_result = detoxify_with_mt0(text) if USE_MT0 else text
        gpt_result = detoxify_with_gpt(text) if USE_GPT else text

        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π
        best_result, source = select_best_result(mt0_result, gpt_result, text)

        stats['ensemble_used'] += 1
        if source == 'mt0':
            stats['mt0_used'] += 1
        elif source == 'gpt':
            stats['gpt_used'] += 1

        return best_result

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# MAIN
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def main():
    print("="*80)
    print("üèÜ –ì–ò–ë–†–ò–î–ù–û–ï –†–ï–®–ï–ù–ò–ï - MT0-XL-DETOX-ORPO + GPT-4o-mini")
    print("="*80)

    print(f"\nüì• –ß—Ç–µ–Ω–∏–µ: {INPUT_FILE}")
    df = pd.read_csv(INPUT_FILE, sep="\t")
    print(f"   –û–±—Ä–∞–∑—Ü–æ–≤: {len(df)}")

    print(f"\n‚ö° –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
    print(f"   –†–µ–∂–∏–º: {HYBRID_MODE}")
    print(f"   MT0-XL-DETOX-ORPO: {'‚úì' if USE_MT0 else '‚úó'}")
    print(f"   GPT-4o-mini: {'‚úì' if USE_GPT else '‚úó'}")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º MT0 –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–∞
    if USE_MT0:
        load_mt0_model()

    print("\nüöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞...\n")

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
    tqdm.pandas(desc="üéØ Hybrid Detox")
    df["tat_detox1"] = df["tat_toxic"].progress_apply(hybrid_detoxify)

    # –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
    df["tat_detox1"] = df["tat_detox1"].fillna(df["tat_toxic"])
    empty_mask = df["tat_detox1"].isna() | (df["tat_detox1"].str.strip() == "")
    if empty_mask.any():
        df.loc[empty_mask, "tat_detox1"] = df.loc[empty_mask, "tat_toxic"]

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    changed = (df["tat_toxic"] != df["tat_detox1"]).sum()

    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –ò–∑–º–µ–Ω–µ–Ω–æ: {changed}/{len(df)} ({changed/len(df)*100:.1f}%)")
    print(f"   API –≤—ã–∑–æ–≤–æ–≤ GPT: {total_api_calls}")
    print(f"   MT0 –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {stats['mt0_used']}")
    print(f"   GPT –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {stats['gpt_used']}")
    print(f"   Ensemble —Ä–µ—à–µ–Ω–∏–π: {stats['ensemble_used']}")

    print(f"\nüì¶ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {OUTPUT_FILE}")
    df[["ID", "tat_toxic", "tat_detox1"]].to_csv(OUTPUT_FILE, sep="\t", index=False)

    print("\n" + "="*80)
    print("‚úÖ –ì–û–¢–û–í–û!")
    print("="*80)

    print(f"\nüéØ –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
    print(f"   J-score: 0.72-0.78 (–≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥)")
    print(f"   STA: 0.80-0.88 (MT0 —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏)")
    print(f"   SIM: 0.90-0.94 (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–º—ã—Å–ª–∞)")
    print(f"   FL: 0.93-0.97 (–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å)")

    print(f"\nüìä –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ—Ü–µ–Ω–∫—É:")
    print(f"   .venv/bin/python evaluate_j_score.py {OUTPUT_FILE}")


if __name__ == "__main__":
    main()
