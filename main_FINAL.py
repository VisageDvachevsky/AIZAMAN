#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üèÜ –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï - –ë–∞–ª–∞–Ω—Å STA √ó SIM √ó FL

–°–¢–†–ê–¢–ï–ì–ò–Ø:
1. Rule-based —É–¥–∞–ª–µ–Ω–∏–µ –º–∞—Ç–æ–≤ (90% —Å–ª—É—á–∞–µ–≤)
2. GPT –¢–û–õ–¨–ö–û –¥–ª—è –æ–±—Ä—É–±–∫–æ–≤ —Å –ñ–ï–°–¢–ö–ò–ú–ò –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
3. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: SIM > STA (–ª—É—á—à–µ –æ—Å—Ç–∞–≤–∏—Ç—å –Ω–µ–º–Ω–æ–≥–æ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ –Ω–æ –≤—ã—Å–æ–∫–∏–π SIM)

–ö–õ–Æ–ß–ï–í–û–ï:
- –ù–ï –º–µ–Ω—è—Ç—å –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—é (—ç, ”ô, “ó –æ—Å—Ç–∞—é—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å!)
- –ù–ï –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
- –ú–ò–ù–ò–ú–£–ú –∏–∑–º–µ–Ω–µ–Ω–∏–π
"""

import re
import os
import pandas as pd
from openai import OpenAI
from tqdm import tqdm
import time
from functools import lru_cache

API_KEY = os.getenv("OPENAI_API_KEY", "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")
MODEL_NAME = "gpt-4o-2024-11-20"
client = OpenAI(api_key=API_KEY)

INPUT_FILE = "dev_inputs.tsv"
OUTPUT_FILE = "submission.tsv"

total_api_calls = 0

# ============================================
# –†–ê–°–®–ò–†–ï–ù–ù–´–ô –°–õ–û–í–ê–†–¨ –¢–û–ö–°–ò–ß–ù–û–°–¢–ò
# ============================================

TOXIC_PATTERNS = [
    # –†—É—Å—Å–∫–∏–µ –º–∞—Ç—ã
    (r'\b[–±–ë]–ª—è[–¥—è–µ—é–∏—å—Ç—å]?\w*', ''),
    (r'\b[–±–ë]–ª[–µ—ç–∏][–Ω—Ç]\w*', ''),
    (r'\b[—Ö–•]—É–π\w*', ''),
    (r'\b[—Ö–•][—É–£][–π–ô–∏–ò–µ–ï—è–Ø]\w*', ''),
    (r'\b[—Ö–•]–µ—Ä\w*', ''),
    (r'\b[–∂–ñ]–æ–ø\w*', ''),
    (r'\b[–µ–ï]–±\w+', ''),
    (r'\b[–∑–ó]–∞–µ–±\w+', ''),
    (r'\b[—Å–°]—É–∫–∞\w*', ''),
    (r'\b[–ø–ü]–∏–∑–¥\w+', ''),
    (r'\b[–Ω–ù]–∞—Ö—É–π\w*', ''),

    # –¢–∞—Ç–∞—Ä—Å–∫–∏–µ –º–∞—Ç—ã
    (r'\b[–∫–ö][—É“Ø–£“Æ][—Ç–¢][–∞-—è”ô”©“Ø“£“ì“õ“ª—ë—ä—åA-Z”ò”®“Æ“¢“í“ö“∫.\-]*', ''),  # –∫—É—Ç*
    (r'\b[–∞–ê][—Ö–•]–º–∞–∫\w*', ''),
    (r'\b[–∞–ê]–Ω–≥—ã—Ä–∞\w*', ''),
    (r'\b[—Ç–¢]–∏–Ω—Ç[”ô—ç]–∫\w*', ''),
    (r'\b[—Ö–•]–∞–π–≤–∞–Ω\w*', ''),
    (r'\b[—á–ß]—É—á–∫–∞\w*', ''),
    (r'\b[–¥–î]—É—Ä–∞–∫\w*', ''),
    (r'\b[–¥–î]–µ–±–∏–ª\w*', ''),
    (r'\b[–∏–ò]–¥–∏–æ—Ç\w*', ''),
    (r'\b[—Å–°]–≤–æ–ª–æ—á—å\w*', ''),
    (r'\b[–º–ú]–∞—Ä[–∂“ó]–∞\w*', ''),
    (r'\b[—Ç–¢]–∏–ª–µ[–ª–∫]?\w*', ''),
    (r'\b[—Å–°]–æ—Å–æ[–ø–º]\w*', ''),
    (r'\b[–∑–ó]–∞–∏–ø–∞\w*', ''),
    (r'\b[–∑–ó]–∞–π–ø\w*', ''),
    (r'\b[–∞–ê]—Ö—É–µ–ª\w*', ''),
    (r'\b[—Ç–¢]–≤–∞—Ä[—å—ä]?\w*', ''),
    (r'\b[–¥–æ–ª–î][–æ–û][–ª–õ]–±–∞[–π—è]\w*', ''),  # –¥–æ–ª–±–∞—è—â–µ—Ä –∏ —Ç.–¥.
]

# –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ñ—Ä–∞–∑—ã
PHRASES = [
    (r'–Ω–∞\s+–∂–æ–ø\w*', ''),
    (r'–≤\s+–∂–æ–ø\w*', ''),
]


def rule_based_detox(text):
    """
    –ì–õ–ê–í–ù–ê–Ø –¥–µ—Ç–æ–∫—Å–∏—Ñ–∏–∫–∞—Ü–∏—è - rule-based

    –ö–†–ò–¢–ò–ß–ù–û: –¢–û–õ–¨–ö–û —É–¥–∞–ª—è–µ–º –º–∞—Ç—ã, –ù–ï –º–µ–Ω—è–µ–º –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—é!
    """
    if not isinstance(text, str) or not text.strip():
        return text

    result = text

    # @user - –º–æ–∂–µ–º —É–¥–∞–ª–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ
    result = re.sub(r'@\w+[,\s]*', '', result)

    # –§—Ä–∞–∑—ã
    for pattern, repl in PHRASES:
        result = re.sub(pattern, repl, result, flags=re.IGNORECASE)

    # –ú–∞—Ç—ã
    for pattern, repl in TOXIC_PATTERNS:
        result = re.sub(pattern, repl, result, flags=re.IGNORECASE)

    # –û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤ (–û–°–¢–û–†–û–ñ–ù–û - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é)
    result = re.sub(r'  +', ' ', result)
    result = re.sub(r'\s+([,!?.;:])', r'\1', result)
    result = re.sub(r'([,!?.;:])\s+([,!?.;:])', r'\1\2', result)  # –î–≤–æ–π–Ω–∞—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è
    result = result.strip()

    # –í–∞–ª–∏–¥–∞—Ü–∏—è: –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π
    if len(result) < len(text) * 0.3:
        # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —É–¥–∞–ª–∏–ª–∏ - –≤–µ—Ä–Ω–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª (–ª—É—á—à–µ —Ç–æ–∫—Å–∏—á–Ω–æ –Ω–æ –≤—ã—Å–æ–∫–∏–π SIM)
        return text

    return result if result else text


def has_truncation(text):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ–±—Ä—É–±–∫–∏ –≤ –∫–æ–Ω—Ü–µ"""
    words = text.strip().split()
    if not words:
        return False

    last_word = words[-1].lower().rstrip('.,!?;:')

    # –û–±—Ä—É–±–æ–∫ –µ—Å–ª–∏ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–≥/—Å–æ—é–∑
    truncation_markers = [
        '–Ω–∞', '–≤', '—Å', '–∫', '–ø–æ', '–æ', '–∑–∞', '–æ—Ç', '—É',
        '–∏', '–∞', '–Ω–æ', '–¥–∞', '–∏–ª–∏', '—á—Ç–æ', '–∫–∞–∫', '—ç—Ç–æ',
        '–æ—á–µ–Ω', '–±–µ–ª”ô–Ω', '–±–µ–ª—ç–Ω', '–¥—ç', '–¥–∞'
    ]

    return last_word in truncation_markers


@lru_cache(maxsize=500)
def gpt_fix_truncation(orig, cleaned):
    """
    GPT –¢–û–õ–¨–ö–û –¥–ª—è –æ–±—Ä—É–±–∫–æ–≤

    –ñ–ï–°–¢–ö–ò–ï –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:
    - –ù–ï –º–µ–Ω—è—Ç—å –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—é (—ç, ”ô, “ó, ”©, “Ø, “£, “ª –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ!)
    - –ù–ï –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
    - –¢–û–õ–¨–ö–û —É–±—Ä–∞—Ç—å –æ–±—Ä—É–±–æ–∫ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏
    """

    if not has_truncation(cleaned):
        return cleaned

    # –ë—ã—Å—Ç—Ä—ã–π fix –±–µ–∑ GPT
    words = cleaned.split()
    if words and len(words) > 1 and words[-1].lower() in ['–Ω–∞', '–≤', '—Å', '–∫', '–æ—á–µ–Ω', '–±–µ–ª”ô–Ω', '–±–µ–ª—ç–Ω', '–¥—ç', '–¥–∞']:
        return ' '.join(words[:-1])

    prompt = f"""–£–±–µ—Ä–∏ –æ–±—Ä—É–±–æ–∫ –≤ –∫–æ–Ω—Ü–µ. –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ï –∏–∑–º–µ–Ω–µ–Ω–∏—è!

–ö–†–ò–¢–ò–ß–ù–û:
- –ù–ï –º–µ–Ω—è–π –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏—é (—ç, ”ô, “ó, ”©, “Ø, “£, “ª –æ—Å—Ç–∞—é—Ç—Å—è –ö–ê–ö –ï–°–¢–¨!)
- –ù–ï –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
- –¢–û–õ–¨–ö–û —É–±–µ—Ä–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø—Ä–µ–¥–ª–æ–≥ –ò–õ–ò –¥–æ–±–∞–≤—å 1 –∫–æ—Ä–æ—Ç–∫–æ–µ —Å–ª–æ–≤–æ –¥–ª—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏

–û—Ä–∏–≥–∏–Ω–∞–ª: {orig}
–° –æ–±—Ä—É–±–∫–æ–º: {cleaned}

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –ë–ï–ó –ø–æ—è—Å–Ω–µ–Ω–∏–π:"""

    global total_api_calls

    try:
        resp = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.15,
            max_tokens=150,
            seed=42
        )

        total_api_calls += 1
        result = resp.choices[0].message.content.strip().strip('"\'`')

        # –í–∞–ª–∏–¥–∞—Ü–∏—è: –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
        if len(result) > len(cleaned) * 1.3:
            return cleaned

        return result if result else cleaned

    except:
        return cleaned


def detox_pipeline(text):
    """
    Pipeline:
    1. Rule-based (–≥–ª–∞–≤–Ω–æ–µ)
    2. GPT —Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—Ä—É–±–∫–æ–≤
    """
    # Step 1: Rule-based
    cleaned = rule_based_detox(text)

    # Step 2: Fix –æ–±—Ä—É–±–∫–æ–≤ (—Ä–µ–¥–∫–æ)
    if has_truncation(cleaned) and len(cleaned) > 10:
        fixed = gpt_fix_truncation(text, cleaned)
        return fixed

    return cleaned


def main():
    print("="*70)
    print("üèÜ –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï - –ë–∞–ª–∞–Ω—Å STA√óSIM√óFL")
    print("="*70)

    print(f"\nüì• Reading: {INPUT_FILE}")
    df = pd.read_csv(INPUT_FILE, sep="\t")
    print(f"   Samples: {len(df)}")

    print(f"\n‚ö° Strategy:")
    print(f"   [1] Rule-based detox (–≥–ª–∞–≤–Ω–æ–µ)")
    print(f"   [2] GPT –¢–û–õ–¨–ö–û –¥–ª—è –æ–±—Ä—É–±–∫–æ–≤ (—Ä–µ–¥–∫–æ)")
    print(f"   [3] –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: SIM > STA")

    print("\nüöÄ Processing...\n")

    tqdm.pandas(desc="üéØ Detox")
    df["tat_detox1"] = df["tat_toxic"].progress_apply(detox_pipeline)

    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    df["tat_detox1"] = df["tat_detox1"].fillna(df["tat_toxic"])
    empty_mask = df["tat_detox1"].isna() | (df["tat_detox1"].str.strip() == "")
    if empty_mask.any():
        df.loc[empty_mask, "tat_detox1"] = df.loc[empty_mask, "tat_toxic"]

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    changed = (df["tat_toxic"] != df["tat_detox1"]).sum()

    length_diffs = []
    for idx in range(len(df)):
        orig = df.iloc[idx]["tat_toxic"]
        detox = df.iloc[idx]["tat_detox1"]
        if len(orig) > 0:
            diff = abs(len(detox) - len(orig)) / len(orig)
            length_diffs.append(diff)

    avg_diff = sum(length_diffs) / len(length_diffs) * 100 if length_diffs else 0

    print(f"\nüìä Statistics:")
    print(f"   Changed: {changed}/{len(df)} ({changed/len(df)*100:.1f}%)")
    print(f"   Avg length Œî: {avg_diff:.1f}%")
    print(f"   GPT calls: {total_api_calls}")

    print(f"\nüì¶ Saving: {OUTPUT_FILE}")
    df[["ID", "tat_toxic", "tat_detox1"]].to_csv(OUTPUT_FILE, sep="\t", index=False)

    print("\n" + "="*70)
    print("‚úÖ –ì–û–¢–û–í–û!")
    print("="*70)

    print(f"\nüéØ –û–∂–∏–¥–∞–Ω–∏—è:")
    print(f"   STA: 0.80-0.85 (–∂–µ—Ä—Ç–≤—É–µ–º –Ω–µ–º–Ω–æ–≥–æ)")
    print(f"   SIM: 0.92-0.95 (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç!)")
    print(f"   FL:  0.96-0.98 (–º–∏–Ω–∏–º—É–º –∏–∑–º–µ–Ω–µ–Ω–∏–π)")
    print(f"   J:   0.70-0.75 ‚Üí –ü–û–ë–ï–î–ê!")

    print(f"\nüìä –ó–∞–ø—É—Å—Ç–∏: .venv/bin/python evaluate_j_score.py")


if __name__ == "__main__":
    main()
