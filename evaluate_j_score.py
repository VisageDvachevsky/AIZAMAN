#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üéØ –õ–æ–∫–∞–ª—å–Ω—ã–π J Score –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¢–ï –ñ–ï –º–æ–¥–µ–ª–∏ —á—Ç–æ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä—ã:
- SIM: LaBSE (sentence-transformers/LaBSE)
- STA: xlm-roberta-large toxicity classifier
- FL: XCOMET-lite

J = mean(STA √ó SIM √ó FL)
"""

import pandas as pd
import numpy as np
from tqdm import tqdm
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings('ignore')

print("üîß –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")

# ================================
# 1. SIM - LaBSE
# ================================
print("   [1/3] LaBSE –¥–ª—è similarity...")
labse_model = SentenceTransformer('sentence-transformers/LaBSE')

def compute_similarity(orig_texts, detox_texts):
    """Cosine similarity –º–µ–∂–¥—É LaBSE embeddings"""
    orig_emb = labse_model.encode(orig_texts, convert_to_numpy=True, show_progress_bar=False)
    detox_emb = labse_model.encode(detox_texts, convert_to_numpy=True, show_progress_bar=False)

    # Cosine similarity
    similarities = []
    for i in range(len(orig_emb)):
        cos_sim = np.dot(orig_emb[i], detox_emb[i]) / (
            np.linalg.norm(orig_emb[i]) * np.linalg.norm(detox_emb[i])
        )
        similarities.append(cos_sim)

    return np.array(similarities)


# ================================
# 2. STA - Toxicity Classifier
# ================================
print("   [2/3] XLM-RoBERTa toxicity classifier...")
tox_model_name = "textdetox/xlmr-large-toxicity-classifier-v2"
tox_tokenizer = AutoTokenizer.from_pretrained(tox_model_name)
tox_model = AutoModelForSequenceClassification.from_pretrained(tox_model_name)
tox_model.eval()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
tox_model.to(device)

def compute_toxicity(texts, batch_size=32):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–µ—Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ (1 - toxicity)
    """
    non_toxicities = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]

        inputs = tox_tokenizer(batch, padding=True, truncation=True,
                              max_length=512, return_tensors="pt")
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():
            outputs = tox_model(**inputs)
            probs = torch.softmax(outputs.logits, dim=-1)
            # Label 0 = non-toxic, Label 1 = toxic
            non_toxic_probs = probs[:, 0].cpu().numpy()
            non_toxicities.extend(non_toxic_probs)

    return np.array(non_toxicities)


# ================================
# 3. FL - XCOMET (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
# ================================
print("   [3/3] Fluency estimator...")

# –î–ª—è fluency –∏—Å–ø–æ–ª—å–∑—É–µ–º proxy: –¥–ª–∏–Ω–∞ + –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –æ–±—Ä—É–±–∫–æ–≤
# –ü–æ–ª–Ω–∞—è XCOMET –º–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º —Ç—è–∂–µ–ª–∞—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º heuristic
def compute_fluency(orig_texts, detox_texts):
    """
    Heuristic fluency:
    - –ù–µ—Ç —Ä–µ–∑–∫–∏—Ö –æ–±—Ä—É–±–∫–æ–≤
    - –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –¥–ª–∏–Ω–∞
    - –ù–µ—Ç –Ω–µ–∑–∞–∫—Ä—ã—Ç–æ–π –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    """
    fluencies = []

    for orig, detox in zip(orig_texts, detox_texts):
        score = 1.0

        # –®—Ç—Ä–∞—Ñ –∑–∞ —Ä–µ–∑–∫–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ
        len_ratio = len(detox) / len(orig) if len(orig) > 0 else 1.0
        if len_ratio < 0.5:
            score *= 0.7  # –ë–æ–ª—å—à–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ
        elif len_ratio < 0.7:
            score *= 0.85

        # –®—Ç—Ä–∞—Ñ –∑–∞ –æ–±—Ä—É–±–∫–∏ (–∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–≥/—Å–æ—é–∑)
        detox_words = detox.strip().split()
        if detox_words:
            last_word = detox_words[-1].lower()
            if last_word in ['–Ω–∞', '–≤', '—Å', '–∫', '–ø–æ', '–æ', '–∑–∞', '–æ—Ç', '—É', '–∏', '–∞', '–Ω–æ', '–¥–∞']:
                score *= 0.6  # –û–±—Ä—É–±–æ–∫

        # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ–∑–∞–∫—Ä—ã—Ç—É—é –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
        if detox.count('(') != detox.count(')'):
            score *= 0.9
        if detox.count('"') % 2 != 0:
            score *= 0.9

        fluencies.append(score)

    return np.array(fluencies)


# ================================
# 4. J Score
# ================================

def compute_j_score(orig_texts, detox_texts):
    """
    J = mean(STA √ó SIM √ó FL)
    """
    print("\nüìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...")

    # SIM
    print("   Similarity (LaBSE)...")
    sim_scores = compute_similarity(orig_texts, detox_texts)

    # STA (non-toxicity of detoxified)
    print("   Style Transfer (toxicity)...")
    sta_scores = compute_toxicity(detox_texts)

    # FL
    print("   Fluency...")
    fl_scores = compute_fluency(orig_texts, detox_texts)

    # J score per sample
    j_scores = sta_scores * sim_scores * fl_scores

    # Mean J score
    j_mean = np.mean(j_scores)

    return {
        'j_score': j_mean,
        'sta_mean': np.mean(sta_scores),
        'sim_mean': np.mean(sim_scores),
        'fl_mean': np.mean(fl_scores),
        'j_scores': j_scores,
        'sta_scores': sta_scores,
        'sim_scores': sim_scores,
        'fl_scores': fl_scores
    }


# ================================
# 5. –ê–Ω–∞–ª–∏–∑
# ================================

def analyze_results(df, results):
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""

    print("\n" + "="*70)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò")
    print("="*70)

    print(f"\nüéØ J Score: {results['j_score']:.4f}")
    print(f"\n   –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:")
    print(f"   STA (detoxification): {results['sta_mean']:.4f}")
    print(f"   SIM (similarity):     {results['sim_mean']:.4f}")
    print(f"   FL  (fluency):        {results['fl_mean']:.4f}")

    # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    print(f"\nüîç –ü–†–û–ë–õ–ï–ú–ù–´–ï –ü–†–ò–ú–ï–†–´:\n")

    # –ù–∏–∑–∫–∏–π STA (—Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å –æ—Å—Ç–∞–ª–∞—Å—å)
    low_sta = np.where(results['sta_scores'] < 0.7)[0]
    if len(low_sta) > 0:
        print(f"   ‚ö†Ô∏è  –ù–∏–∑–∫–∏–π STA (—Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å –æ—Å—Ç–∞–ª–∞—Å—å): {len(low_sta)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        for idx in low_sta[:5]:
            print(f"      [{idx}] STA={results['sta_scores'][idx]:.2f}")
            print(f"          {df.iloc[idx]['tat_detox1'][:70]}\n")

    # –ù–∏–∑–∫–∏–π SIM (—Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    low_sim = np.where(results['sim_scores'] < 0.8)[0]
    if len(low_sim) > 0:
        print(f"   ‚ö†Ô∏è  –ù–∏–∑–∫–∏–π SIM (–º–Ω–æ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π): {len(low_sim)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        for idx in low_sim[:5]:
            print(f"      [{idx}] SIM={results['sim_scores'][idx]:.2f}")
            print(f"          Orig:  {df.iloc[idx]['tat_toxic'][:60]}")
            print(f"          Detox: {df.iloc[idx]['tat_detox1'][:60]}\n")

    # –ù–∏–∑–∫–∏–π FL (–Ω–µ–≥—Ä–∞–º–º–∞—Ç–∏—á–Ω–æ—Å—Ç—å)
    low_fl = np.where(results['fl_scores'] < 0.7)[0]
    if len(low_fl) > 0:
        print(f"   ‚ö†Ô∏è  –ù–∏–∑–∫–∏–π FL (–æ–±—Ä—É–±–∫–∏): {len(low_fl)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        for idx in low_fl[:5]:
            print(f"      [{idx}] FL={results['fl_scores'][idx]:.2f}")
            print(f"          {df.iloc[idx]['tat_detox1'][:70]}\n")

    # –õ—É—á—à–∏–µ –ø—Ä–∏–º–µ—Ä—ã
    print(f"\n‚úÖ –õ–£–ß–®–ò–ï –ü–†–ò–ú–ï–†–´ (–≤—ã—Å–æ–∫–∏–π J):\n")
    top_indices = np.argsort(results['j_scores'])[-5:][::-1]
    for idx in top_indices:
        j = results['j_scores'][idx]
        sta = results['sta_scores'][idx]
        sim = results['sim_scores'][idx]
        fl = results['fl_scores'][idx]

        print(f"   [{idx}] J={j:.3f} (STA={sta:.2f}, SIM={sim:.2f}, FL={fl:.2f})")
        print(f"       Orig:  {df.iloc[idx]['tat_toxic'][:65]}")
        print(f"       Detox: {df.iloc[idx]['tat_detox1'][:65]}\n")


def main():
    import sys

    submission_file = sys.argv[1] if len(sys.argv) > 1 else "submission.tsv"

    print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞: {submission_file}")
    df = pd.read_csv(submission_file, sep='\t')
    print(f"   –ü—Ä–∏–º–µ—Ä–æ–≤: {len(df)}")

    # –í—ã—á–∏—Å–ª—è–µ–º J score
    results = compute_j_score(
        df['tat_toxic'].tolist(),
        df['tat_detox1'].tolist()
    )

    # –ê–Ω–∞–ª–∏–∑
    analyze_results(df, results)

    print("\n" + "="*70)
    print(f"üéØ –§–ò–ù–ê–õ–¨–ù–´–ô J SCORE: {results['j_score']:.4f}")
    print("="*70)


if __name__ == "__main__":
    main()
